{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "from mobilenetv3 import MobileNetV3_forFPN, MobileNetV3, load_pretrained_fpn\n",
    "from tests import test_loaded_weights, compare_output\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "mn3_fpn = MobileNetV3_forFPN()\n",
    "mn3_fpn = load_pretrained_fpn(mn3_fpn,\\\n",
    "                              '/home/davidyuk/Projects/backbones/pytorch-mobilenet-v3/mobilenetv3_small_67.4.pth.tar')\n",
    "with torch.no_grad():\n",
    "    print(mn3_fpn.forward(torch.zeros(1,3,224,224)).size())\n",
    "# tuple(mn3_fpn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed\n"
     ]
    }
   ],
   "source": [
    "test_loaded_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers_os4.0.0.weight\n",
      "layers_os4.0.1.weight\n",
      "layers_os4.0.1.bias\n",
      "layers_os4.1.conv.0.weight\n",
      "layers_os4.1.conv.1.weight\n",
      "layers_os4.1.conv.1.bias\n",
      "layers_os4.1.conv.3.weight\n",
      "layers_os4.1.conv.4.weight\n",
      "layers_os4.1.conv.4.bias\n",
      "layers_os4.1.conv.5.fc.0.weight\n",
      "layers_os4.1.conv.5.fc.2.weight\n",
      "layers_os4.1.conv.7.weight\n",
      "layers_os4.1.conv.8.weight\n",
      "layers_os4.1.conv.8.bias\n",
      "layers_os8.0.conv.0.weight\n",
      "layers_os8.0.conv.1.weight\n",
      "layers_os8.0.conv.1.bias\n",
      "layers_os8.0.conv.3.weight\n",
      "layers_os8.0.conv.4.weight\n",
      "layers_os8.0.conv.4.bias\n",
      "layers_os8.0.conv.7.weight\n",
      "layers_os8.0.conv.8.weight\n",
      "layers_os8.0.conv.8.bias\n",
      "layers_os8.1.conv.0.weight\n",
      "layers_os8.1.conv.1.weight\n",
      "layers_os8.1.conv.1.bias\n",
      "layers_os8.1.conv.3.weight\n",
      "layers_os8.1.conv.4.weight\n",
      "layers_os8.1.conv.4.bias\n",
      "layers_os8.1.conv.7.weight\n",
      "layers_os8.1.conv.8.weight\n",
      "layers_os8.1.conv.8.bias\n",
      "layers_os16.0.conv.0.weight\n",
      "layers_os16.0.conv.1.weight\n",
      "layers_os16.0.conv.1.bias\n",
      "layers_os16.0.conv.3.weight\n",
      "layers_os16.0.conv.4.weight\n",
      "layers_os16.0.conv.4.bias\n",
      "layers_os16.0.conv.5.fc.0.weight\n",
      "layers_os16.0.conv.5.fc.2.weight\n",
      "layers_os16.0.conv.7.weight\n",
      "layers_os16.0.conv.8.weight\n",
      "layers_os16.0.conv.8.bias\n",
      "layers_os16.1.conv.0.weight\n",
      "layers_os16.1.conv.1.weight\n",
      "layers_os16.1.conv.1.bias\n",
      "layers_os16.1.conv.3.weight\n",
      "layers_os16.1.conv.4.weight\n",
      "layers_os16.1.conv.4.bias\n",
      "layers_os16.1.conv.5.fc.0.weight\n",
      "layers_os16.1.conv.5.fc.2.weight\n",
      "layers_os16.1.conv.7.weight\n",
      "layers_os16.1.conv.8.weight\n",
      "layers_os16.1.conv.8.bias\n",
      "layers_os16.2.conv.0.weight\n",
      "layers_os16.2.conv.1.weight\n",
      "layers_os16.2.conv.1.bias\n",
      "layers_os16.2.conv.3.weight\n",
      "layers_os16.2.conv.4.weight\n",
      "layers_os16.2.conv.4.bias\n",
      "layers_os16.2.conv.5.fc.0.weight\n",
      "layers_os16.2.conv.5.fc.2.weight\n",
      "layers_os16.2.conv.7.weight\n",
      "layers_os16.2.conv.8.weight\n",
      "layers_os16.2.conv.8.bias\n",
      "layers_os16.3.conv.0.weight\n",
      "layers_os16.3.conv.1.weight\n",
      "layers_os16.3.conv.1.bias\n",
      "layers_os16.3.conv.3.weight\n",
      "layers_os16.3.conv.4.weight\n",
      "layers_os16.3.conv.4.bias\n",
      "layers_os16.3.conv.5.fc.0.weight\n",
      "layers_os16.3.conv.5.fc.2.weight\n",
      "layers_os16.3.conv.7.weight\n",
      "layers_os16.3.conv.8.weight\n",
      "layers_os16.3.conv.8.bias\n",
      "layers_os16.4.conv.0.weight\n",
      "layers_os16.4.conv.1.weight\n",
      "layers_os16.4.conv.1.bias\n",
      "layers_os16.4.conv.3.weight\n",
      "layers_os16.4.conv.4.weight\n",
      "layers_os16.4.conv.4.bias\n",
      "layers_os16.4.conv.5.fc.0.weight\n",
      "layers_os16.4.conv.5.fc.2.weight\n",
      "layers_os16.4.conv.7.weight\n",
      "layers_os16.4.conv.8.weight\n",
      "layers_os16.4.conv.8.bias\n",
      "layers_os32.0.conv.0.weight\n",
      "layers_os32.0.conv.1.weight\n",
      "layers_os32.0.conv.1.bias\n",
      "layers_os32.0.conv.3.weight\n",
      "layers_os32.0.conv.4.weight\n",
      "layers_os32.0.conv.4.bias\n",
      "layers_os32.0.conv.5.fc.0.weight\n",
      "layers_os32.0.conv.5.fc.2.weight\n",
      "layers_os32.0.conv.7.weight\n",
      "layers_os32.0.conv.8.weight\n",
      "layers_os32.0.conv.8.bias\n",
      "layers_os32.1.conv.0.weight\n",
      "layers_os32.1.conv.1.weight\n",
      "layers_os32.1.conv.1.bias\n",
      "layers_os32.1.conv.3.weight\n",
      "layers_os32.1.conv.4.weight\n",
      "layers_os32.1.conv.4.bias\n",
      "layers_os32.1.conv.5.fc.0.weight\n",
      "layers_os32.1.conv.5.fc.2.weight\n",
      "layers_os32.1.conv.7.weight\n",
      "layers_os32.1.conv.8.weight\n",
      "layers_os32.1.conv.8.bias\n",
      "layers_os32.2.conv.0.weight\n",
      "layers_os32.2.conv.1.weight\n",
      "layers_os32.2.conv.1.bias\n",
      "layers_os32.2.conv.3.weight\n",
      "layers_os32.2.conv.4.weight\n",
      "layers_os32.2.conv.4.bias\n",
      "layers_os32.2.conv.5.fc.0.weight\n",
      "layers_os32.2.conv.5.fc.2.weight\n",
      "layers_os32.2.conv.7.weight\n",
      "layers_os32.2.conv.8.weight\n",
      "layers_os32.2.conv.8.bias\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in mn3_fpn.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
    "\n",
    "def mobilenetV3_fpn_backbone(pretrained=False):\n",
    "    backbone = MobileNetV3_forFPN()\n",
    "    \n",
    "    if pretrained:\n",
    "        load_pretrained_fpn(backbone,\\\n",
    "                              '/home/davidyuk/Projects/backbones/pytorch-mobilenet-v3/mobilenetv3_small_67.4.pth.tar')\n",
    "        \n",
    "    return_layers = {'layers_os4': 0, 'layers_os8': 1, 'layers_os16': 2, 'layers_os32': 3}\n",
    "\n",
    "    in_channels_list = [16, 24, 48, 96]\n",
    "    out_channels = 100\n",
    "    return BackboneWithFPN(backbone, return_layers, in_channels_list, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 56, 56])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb3_ = mobilenetV3_fpn_backbone(pretrained=True)\n",
    "mb3_.forward(torch.ones(1,3,224,224))[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone (nn.Module): the network used to compute the features for the model.\n",
    "    It should contain a out_channels attribute, which indicates the number of output\n",
    "    channels that each feature map has (and it should be the same for all feature maps).\n",
    "    The backbone should return a single Tensor or and OrderedDict[Tensor].\n",
    "num_classes (int): number of output classes of the model (including the background).\n",
    "    If box_predictor is specified, num_classes should be None.\n",
    "min_size (int): minimum size of the image to be rescaled before feeding it to the backbone\n",
    "max_size (int): maximum size of the image to be rescaled before feeding it to the backbone\n",
    "image_mean (Tuple[float, float, float]): mean values used for input normalization.\n",
    "    They are generally the mean values of the dataset on which the backbone has been trained\n",
    "    on\n",
    "image_std (Tuple[float, float, float]): std values used for input normalization.\n",
    "    They are generally the std values of the dataset on which the backbone has been trained on\n",
    "rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n",
    "    maps.\n",
    "rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN\n",
    "rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n",
    "rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n",
    "rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n",
    "rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n",
    "rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n",
    "rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
    "    considered as positive during training of the RPN.\n",
    "rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n",
    "    considered as negative during training of the RPN.\n",
    "rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n",
    "    for computing the loss\n",
    "rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training\n",
    "    of the RPN\n",
    "box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
    "    the locations indicated by the bounding boxes\n",
    "box_head (nn.Module): module that takes the cropped feature maps as input\n",
    "box_predictor (nn.Module): module that takes the output of box_head and returns the\n",
    "    classification logits and box regression deltas.\n",
    "box_score_thresh (float): during inference, only return proposals with a classification score\n",
    "    greater than box_score_thresh\n",
    "box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n",
    "box_detections_per_img (int): maximum number of detections per image, for all classes.\n",
    "box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be\n",
    "    considered as positive during training of the classification head\n",
    "box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be\n",
    "    considered as negative during training of the classification head\n",
    "box_batch_size_per_image (int): number of proposals that are sampled during training of the\n",
    "    classification head\n",
    "box_positive_fraction (float): proportion of positive proposals in a mini-batch during training\n",
    "    of the classification head\n",
    "bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the\n",
    "    bounding boxes\n",
    "mask_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
    "     the locations indicated by the bounding boxes, which will be used for the mask head.\n",
    "mask_head (nn.Module): module that takes the cropped feature maps as input\n",
    "mask_predictor (nn.Module): module that takes the output of the mask_head and returns the\n",
    "    segmentation mask logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.mask_rcnn import MaskRCNN\n",
    "def maskrcnn_mobileNetV3_fpn(num_classes=3, pretrained_backbone=True):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
