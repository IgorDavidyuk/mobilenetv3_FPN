{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "from mobilenetv3 import MobileNetV3_forFPN, MobileNetV3, load_pretrained_fpn\n",
    "from tests import test_loaded_weights, compare_output\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "mn3_fpn = MobileNetV3_forFPN()\n",
    "mn3_fpn = load_pretrained_fpn(mn3_fpn,\\\n",
    "                              '/home/davidyuk/Projects/backbones/pytorch-mobilenet-v3/mobilenetv3_small_67.4.pth.tar')\n",
    "with torch.no_grad():\n",
    "    print(mn3_fpn.forward(torch.zeros(1,3,224,224)).size())\n",
    "# tuple(mn3_fpn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passed\n"
     ]
    }
   ],
   "source": [
    "test_loaded_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers_os4.0.0.weight\n",
      "layers_os4.0.1.weight\n",
      "layers_os4.0.1.bias\n",
      "layers_os4.1.conv.0.weight\n",
      "layers_os4.1.conv.1.weight\n",
      "layers_os4.1.conv.1.bias\n",
      "layers_os4.1.conv.3.weight\n",
      "layers_os4.1.conv.4.weight\n",
      "layers_os4.1.conv.4.bias\n",
      "layers_os4.1.conv.5.fc.0.weight\n",
      "layers_os4.1.conv.5.fc.2.weight\n",
      "layers_os4.1.conv.7.weight\n",
      "layers_os4.1.conv.8.weight\n",
      "layers_os4.1.conv.8.bias\n",
      "layers_os8.0.conv.0.weight\n",
      "layers_os8.0.conv.1.weight\n",
      "layers_os8.0.conv.1.bias\n",
      "layers_os8.0.conv.3.weight\n",
      "layers_os8.0.conv.4.weight\n",
      "layers_os8.0.conv.4.bias\n",
      "layers_os8.0.conv.7.weight\n",
      "layers_os8.0.conv.8.weight\n",
      "layers_os8.0.conv.8.bias\n",
      "layers_os8.1.conv.0.weight\n",
      "layers_os8.1.conv.1.weight\n",
      "layers_os8.1.conv.1.bias\n",
      "layers_os8.1.conv.3.weight\n",
      "layers_os8.1.conv.4.weight\n",
      "layers_os8.1.conv.4.bias\n",
      "layers_os8.1.conv.7.weight\n",
      "layers_os8.1.conv.8.weight\n",
      "layers_os8.1.conv.8.bias\n",
      "layers_os16.0.conv.0.weight\n",
      "layers_os16.0.conv.1.weight\n",
      "layers_os16.0.conv.1.bias\n",
      "layers_os16.0.conv.3.weight\n",
      "layers_os16.0.conv.4.weight\n",
      "layers_os16.0.conv.4.bias\n",
      "layers_os16.0.conv.5.fc.0.weight\n",
      "layers_os16.0.conv.5.fc.2.weight\n",
      "layers_os16.0.conv.7.weight\n",
      "layers_os16.0.conv.8.weight\n",
      "layers_os16.0.conv.8.bias\n",
      "layers_os16.1.conv.0.weight\n",
      "layers_os16.1.conv.1.weight\n",
      "layers_os16.1.conv.1.bias\n",
      "layers_os16.1.conv.3.weight\n",
      "layers_os16.1.conv.4.weight\n",
      "layers_os16.1.conv.4.bias\n",
      "layers_os16.1.conv.5.fc.0.weight\n",
      "layers_os16.1.conv.5.fc.2.weight\n",
      "layers_os16.1.conv.7.weight\n",
      "layers_os16.1.conv.8.weight\n",
      "layers_os16.1.conv.8.bias\n",
      "layers_os16.2.conv.0.weight\n",
      "layers_os16.2.conv.1.weight\n",
      "layers_os16.2.conv.1.bias\n",
      "layers_os16.2.conv.3.weight\n",
      "layers_os16.2.conv.4.weight\n",
      "layers_os16.2.conv.4.bias\n",
      "layers_os16.2.conv.5.fc.0.weight\n",
      "layers_os16.2.conv.5.fc.2.weight\n",
      "layers_os16.2.conv.7.weight\n",
      "layers_os16.2.conv.8.weight\n",
      "layers_os16.2.conv.8.bias\n",
      "layers_os16.3.conv.0.weight\n",
      "layers_os16.3.conv.1.weight\n",
      "layers_os16.3.conv.1.bias\n",
      "layers_os16.3.conv.3.weight\n",
      "layers_os16.3.conv.4.weight\n",
      "layers_os16.3.conv.4.bias\n",
      "layers_os16.3.conv.5.fc.0.weight\n",
      "layers_os16.3.conv.5.fc.2.weight\n",
      "layers_os16.3.conv.7.weight\n",
      "layers_os16.3.conv.8.weight\n",
      "layers_os16.3.conv.8.bias\n",
      "layers_os16.4.conv.0.weight\n",
      "layers_os16.4.conv.1.weight\n",
      "layers_os16.4.conv.1.bias\n",
      "layers_os16.4.conv.3.weight\n",
      "layers_os16.4.conv.4.weight\n",
      "layers_os16.4.conv.4.bias\n",
      "layers_os16.4.conv.5.fc.0.weight\n",
      "layers_os16.4.conv.5.fc.2.weight\n",
      "layers_os16.4.conv.7.weight\n",
      "layers_os16.4.conv.8.weight\n",
      "layers_os16.4.conv.8.bias\n",
      "layers_os32.0.conv.0.weight\n",
      "layers_os32.0.conv.1.weight\n",
      "layers_os32.0.conv.1.bias\n",
      "layers_os32.0.conv.3.weight\n",
      "layers_os32.0.conv.4.weight\n",
      "layers_os32.0.conv.4.bias\n",
      "layers_os32.0.conv.5.fc.0.weight\n",
      "layers_os32.0.conv.5.fc.2.weight\n",
      "layers_os32.0.conv.7.weight\n",
      "layers_os32.0.conv.8.weight\n",
      "layers_os32.0.conv.8.bias\n",
      "layers_os32.1.conv.0.weight\n",
      "layers_os32.1.conv.1.weight\n",
      "layers_os32.1.conv.1.bias\n",
      "layers_os32.1.conv.3.weight\n",
      "layers_os32.1.conv.4.weight\n",
      "layers_os32.1.conv.4.bias\n",
      "layers_os32.1.conv.5.fc.0.weight\n",
      "layers_os32.1.conv.5.fc.2.weight\n",
      "layers_os32.1.conv.7.weight\n",
      "layers_os32.1.conv.8.weight\n",
      "layers_os32.1.conv.8.bias\n",
      "layers_os32.2.conv.0.weight\n",
      "layers_os32.2.conv.1.weight\n",
      "layers_os32.2.conv.1.bias\n",
      "layers_os32.2.conv.3.weight\n",
      "layers_os32.2.conv.4.weight\n",
      "layers_os32.2.conv.4.bias\n",
      "layers_os32.2.conv.5.fc.0.weight\n",
      "layers_os32.2.conv.5.fc.2.weight\n",
      "layers_os32.2.conv.7.weight\n",
      "layers_os32.2.conv.8.weight\n",
      "layers_os32.2.conv.8.bias\n"
     ]
    }
   ],
   "source": [
    "for name, parameter in mn3_fpn.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
    "\n",
    "def mobilenetV3_fpn_backbone(pretrained=False):\n",
    "    backbone = MobileNetV3_forFPN()\n",
    "    \n",
    "    if pretrained:\n",
    "        load_pretrained_fpn(backbone,\\\n",
    "                              '/home/davidyuk/Projects/backbones/pytorch-mobilenet-v3/mobilenetv3_small_67.4.pth.tar')\n",
    "        \n",
    "    return_layers = {'layers_os4': 0, 'layers_os8': 1, 'layers_os16': 2, 'layers_os32': 3}\n",
    "\n",
    "    in_channels_list = [16, 24, 48, 96]\n",
    "    out_channels = 100\n",
    "    return BackboneWithFPN(backbone, return_layers, in_channels_list, out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 56, 56])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb3_ = mobilenetV3_fpn_backbone(pretrained=True)\n",
    "mb3_.forward(torch.ones(1,3,224,224))[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone (nn.Module): the network used to compute the features for the model.\n",
    "    It should contain a out_channels attribute, which indicates the number of output\n",
    "    channels that each feature map has (and it should be the same for all feature maps).\n",
    "    The backbone should return a single Tensor or and OrderedDict[Tensor].\n",
    "num_classes (int): number of output classes of the model (including the background).\n",
    "    If box_predictor is specified, num_classes should be None.\n",
    "min_size (int): minimum size of the image to be rescaled before feeding it to the backbone\n",
    "max_size (int): maximum size of the image to be rescaled before feeding it to the backbone\n",
    "image_mean (Tuple[float, float, float]): mean values used for input normalization.\n",
    "    They are generally the mean values of the dataset on which the backbone has been trained\n",
    "    on\n",
    "image_std (Tuple[float, float, float]): std values used for input normalization.\n",
    "    They are generally the std values of the dataset on which the backbone has been trained on\n",
    "rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n",
    "    maps.\n",
    "rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN\n",
    "rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n",
    "rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n",
    "rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n",
    "rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n",
    "rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n",
    "rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
    "    considered as positive during training of the RPN.\n",
    "rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n",
    "    considered as negative during training of the RPN.\n",
    "rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n",
    "    for computing the loss\n",
    "rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training\n",
    "    of the RPN\n",
    "box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
    "    the locations indicated by the bounding boxes\n",
    "box_head (nn.Module): module that takes the cropped feature maps as input\n",
    "box_predictor (nn.Module): module that takes the output of box_head and returns the\n",
    "    classification logits and box regression deltas.\n",
    "box_score_thresh (float): during inference, only return proposals with a classification score\n",
    "    greater than box_score_thresh\n",
    "box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n",
    "box_detections_per_img (int): maximum number of detections per image, for all classes.\n",
    "box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be\n",
    "    considered as positive during training of the classification head\n",
    "box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be\n",
    "    considered as negative during training of the classification head\n",
    "box_batch_size_per_image (int): number of proposals that are sampled during training of the\n",
    "    classification head\n",
    "box_positive_fraction (float): proportion of positive proposals in a mini-batch during training\n",
    "    of the classification head\n",
    "bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the\n",
    "    bounding boxes\n",
    "mask_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
    "     the locations indicated by the bounding boxes, which will be used for the mask head.\n",
    "mask_head (nn.Module): module that takes the cropped feature maps as input\n",
    "mask_predictor (nn.Module): module that takes the output of the mask_head and returns the\n",
    "    segmentation mask logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 0\n",
    "import torch, torchvision\n",
    "from MASKRCNN_creator import maskrcnn_mobileNetV3_fpn\n",
    "\n",
    "chkp_path = '/home/davidyuk/Projects/backbones/pytorch-mobilenet-v3/mobilenetv3_small_67.4.pth.tar'\n",
    "model = maskrcnn_mobileNetV3_fpn(num_classes=3, backbone_chkp=chkp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeatMap size aware heads and necks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_channels = backbone.out_channels\n",
    "\n",
    "\n",
    "from .generalized_rcnn import GeneralizedRCNN\n",
    ", , \n",
    "from .roi_heads import RoIHeads\n",
    "\n",
    "from .rpn import AnchorGenerator\n",
    "anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "rpn_anchor_generator = AnchorGenerator(\n",
    "    anchor_sizes, aspect_ratios\n",
    ")\n",
    "\n",
    "from .rpn import RPNHead\n",
    "rpn_head = RPNHead(\n",
    "    out_channels, rpn_anchor_generator.num_anchors_per_location()[0]\n",
    ")\n",
    "\n",
    "rpn_pre_nms_top_n = dict(training=rpn_pre_nms_top_n_train, testing=rpn_pre_nms_top_n_test)\n",
    "rpn_post_nms_top_n = dict(training=rpn_post_nms_top_n_train, testing=rpn_post_nms_top_n_test)\n",
    "\n",
    "from .rpn import RegionProposalNetwork\n",
    "rpn = RegionProposalNetwork(\n",
    "    rpn_anchor_generator, rpn_head,\n",
    "    rpn_fg_iou_thresh, rpn_bg_iou_thresh,\n",
    "    rpn_batch_size_per_image, rpn_positive_fraction,\n",
    "    rpn_pre_nms_top_n, rpn_post_nms_top_n, rpn_nms_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster RCNN heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if box_roi_pool is None:\n",
    "    box_roi_pool = MultiScaleRoIAlign(\n",
    "        featmap_names=[0, 1, 2, 3],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2)\n",
    "\n",
    "if box_head is None:\n",
    "    resolution = box_roi_pool.output_size[0]\n",
    "    representation_size = 1024\n",
    "    box_head = TwoMLPHead(\n",
    "        out_channels * resolution ** 2,\n",
    "        representation_size)\n",
    "\n",
    "if box_predictor is None:\n",
    "    representation_size = 1024\n",
    "    box_predictor = FastRCNNPredictor(\n",
    "        representation_size,\n",
    "        num_classes)\n",
    "\n",
    "roi_heads = RoIHeads(\n",
    "    # Box\n",
    "    box_roi_pool, box_head, box_predictor,\n",
    "    box_fg_iou_thresh, box_bg_iou_thresh,\n",
    "    box_batch_size_per_image, box_positive_fraction,\n",
    "    bbox_reg_weights,\n",
    "    box_score_thresh, box_nms_thresh, box_detections_per_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MASK RCNN heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.mask_rcnn import MultiScaleRoIAlign\n",
    "mask_roi_pool = MultiScaleRoIAlign(\n",
    "    featmap_names=[0, 1, 2, 3],\n",
    "    output_size=14,\n",
    "    sampling_ratio=2)\n",
    "\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNHeads\n",
    "mask_layers = (256, 256, 256, 256)\n",
    "mask_dilation = 1\n",
    "mask_head = MaskRCNNHeads(out_channels, mask_layers, mask_dilation)\n",
    "\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "mask_predictor_in_channels = 256  # == mask_layers[-1]\n",
    "mask_dim_reduced = 256\n",
    "mask_predictor = MaskRCNNPredictor(mask_predictor_in_channels,\n",
    "                                   mask_dim_reduced, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keypoints RCNN heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1600) must match the existing size (1333) at non-singleton dimension 2.  Target sizes: [3, 800, 1600].  Tensor sizes: [3, 666, 1333]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-413efca725e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cuda9_passport/lib/python3.6/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In training mode, targets should be passed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0moriginal_image_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cuda9_passport/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/envs/cuda9_passport/lib/python3.6/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1600) must match the existing size (1333) at non-singleton dimension 2.  Target sizes: [3, 800, 1600].  Tensor sizes: [3, 666, 1333]"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "inp = torch.ones(1,3,800,1600)\n",
    "out = model.forward(inp)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
